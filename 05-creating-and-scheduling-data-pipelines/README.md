# Creating and Scheduling Data Pipelines
![DataModel](data-model.png)
<br>

### 1. เข้าไปที่ 
```sh
cd 05-creating-and-scheduling-data-pipelines
```

### 2. ถ้าใช้งานระบบที่เป็น Linux ให้เรารันคำสั่งด้านล่างนี้ก่อน

```sh
mkdir -p ./dags ./logs ./plugins
echo -e "AIRFLOW_UID=$(id -u)" > .env
```

### 3. หลังจากนั้นให้รัน

```sh
docker-compose up
```

เราจะสามารถเข้าไปที่หน้า Airflow UI ได้ที่ port 8080

เสร็จแล้วให้คัดลอกโฟลเดอร์ `data` ที่เตรียมไว้ข้างนอกสุด เข้ามาใส่ในโฟลเดอร์ `dags` เพื่อที่ Airflow จะได้เห็นไฟล์ข้อมูลเหล่านี้ แล้วจึงค่อยทำโปรเจคต่อ

**หมายเหตุ:** จริง ๆ แล้วเราสามารถเอาโฟลเดอร์ `data` ไว้ที่ไหนก็ได้ที่ Airflow ที่เรารันเข้าถึงได้ แต่เพื่อความง่ายสำหรับโปรเจคนี้ เราจะนำเอาโฟลเดอร์ `data` ไว้ในโฟลเดอร์ `dags` เลย

### 4. ทำการตั้งค่าการเชื่อมต่อใน Postgres Port 8088 
(1667409666493.jpg)
<br>

### 5. ทำการเชื่อมต่อ Postgres กับ Airflow จากเมนู Admin>Connections และกรอกข้อมูลให้สอดคล้องกับการตั้งค่าใน Postgres
(1667409686644.jpg)
<br>

### 6. สร้างไฟล์ .py ในการเขียนคำสั่งต่างๆ และกำหนดระยะเวลาหรือช่วงเวลาในการรันคำสั่ง
(1667409774871.jpg)
<br>

โดยกราฟขั้นตอนในการรันจะได้ดังนี้
(S__2727976.jpg)
<br>